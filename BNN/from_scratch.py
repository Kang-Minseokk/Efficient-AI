import torch
import os
from models.reactnet_backbone import birealnet18
from models.MLP import get_mlp
import torch.nn as nn
import torch.optim as optim
from datasets import get_dataset
from utils import *
from get_args import get_args
from models.VGG import *
from models.MLP import *
from models.base import *
from models.mask_generator import get_mask_generator
import torch.nn.functional as F


# from torchvision import datasets, transforms
# def test_activation_saving(model, device, train_loader):
#     model.eval()  # í‰ê°€ ëª¨ë“œ
#     dummy_input = torch.randn(2, 28, 28)  # ì‘ì€ ë°°ì¹˜, MNIST ì‚¬ì´ì¦ˆ
#     dummy_input = dummy_input.view(2, -1)  # Flatten: (2, 784)

#     with torch.no_grad():
#         output = model(dummy_input)

#     print(f"ğŸ” Activation ì €ì¥ ê°œìˆ˜: {len(model.activations)}")
#     for i, act in enumerate(model.activations):
#         print(f"[{i}] Activation shape: {act.shape}")


# ë©”ì¸ ì½”ë“œ
def main():        
    # MNIST ë°ì´í„°ì…‹ ì‚¬ìš©
    train_set, test_set, loss_function = get_dataset("MNIST", test=True)
    
    train_loader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True)
    test_loader = torch.utils.data.DataLoader(test_set, batch_size=128)
    
    # 2. ëª¨ë¸ ìƒì„±
    # ì´ë¯¸ì§€ í¬ê¸°ì™€ ì±„ë„ ìˆ˜ì— ë§ê²Œ ì„¤ì • (CIFAR-10ì€ 32x32x3ì´ì§€ë§Œ 64x64ë¡œ ë³€í™˜)
    model = get_mlp(
        in_shape=(28, 1),     # 64x64 í¬ê¸°, 3ì±„ë„ ì´ë¯¸ì§€
        hid_dim=512,          # ì€ë‹‰ì¸µ ë‰´ëŸ° ìˆ˜
        out_dim=10,           # CIFAR-10ì€ 10ê°œ í´ë˜ìŠ¤
        depth=2,              # 1ê°œì˜ ì€ë‹‰ì¸µ
        batch_norm=True,      # ë°°ì¹˜ ì •ê·œí™” ì‚¬ìš©
        b_weight=True,        # ì´ì§„ ê°€ì¤‘ì¹˜ ì‚¬ìš© ì•ˆ í•¨
        b_act=True           # ì´ì§„ í™œì„±í™” í•¨ìˆ˜ ì‚¬ìš© ì•ˆ í•¨
    )
    print(f"ğŸ‘€ model í´ë˜ìŠ¤: {model.__class__}")
    print(model)
    # 3. ì†ì‹¤ í•¨ìˆ˜ì™€ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ ì •ì˜
    criterion = nn.CrossEntropyLoss()
    lr = 0.001 # í•™ìŠµë¥  ì§ì ‘ ì‚¬ìš©
    # optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    # 4. í•™ìŠµ ë£¨í”„    
    num_epochs = 1000
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    for epoch in range(num_epochs):
        # test_activation_saving(model, device, train_loader)
        # í•™ìŠµ ëª¨ë“œ ì„¤ì •
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        
        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            
            outputs = model(inputs)
            loss = criterion(outputs, labels)            

            # === ì´ì§„ ì—…ë°ì´íŠ¸ëŠ” ê·¸ ë‹¤ìŒì— ===
            probs = torch.softmax(outputs, dim=1)            
            one_hot = F.one_hot(labels, num_classes=10).float()
            error_sign = torch.sign(probs - one_hot)  # ì˜¤ì°¨ ë°©í–¥ ê³„ì‚°            
            activation = model.activations[0]  # -1 ë˜ëŠ” 1     
          
            with torch.no_grad():                
                weight_layer = model.classifier[-1]
                weight = weight_layer.weight.data   # shape: (C, H), ex: (10, 512)
                
                # 1. Î”L ê³„ì‚°
                approx_grad = activation.T @ error_sign   # shape: (H, C)
                deltaL = approx_grad.T * (1 - 2 * weight) # shape: (C, H)
                
                # 2. Î”L â‰¤ 0 ì¸ ê²ƒë§Œ í›„ë³´ë¡œ ì‚¬ìš©
                mask = (deltaL <= 0)
                if mask.any():
                    deltaL_candidates = deltaL[mask]

                    # 3. Î”L ì •ê·œí™” (LayerNorm ìŠ¤íƒ€ì¼)
                    mu = deltaL_candidates.mean()
                    sigma = deltaL_candidates.std(unbiased=False) + 1e-8
                    hat_deltaL = (deltaL_candidates - mu) / sigma

                    # 4. z ê³„ì‚° (í•™ìŠµ ê°€ëŠ¥í•œ gamma, beta ëŒ€ì‹  ìƒìˆ˜)
                    gamma = 1.0
                    beta = -(0.0 - mu) / sigma  # Î”L_target = 0 ê¸°ì¤€
                    z = gamma * hat_deltaL + beta

                    # 5. Laplace ê¸°ë°˜ í™•ë¥  ê³„ì‚°
                    b = 0.3  # Laplace scale (ë‚®ê²Œ ì¡°ì •)
                    p_candidates = torch.exp(-torch.abs(z) / b)

                    # 6. ì „ì²´ í™•ë¥  ë°°ì—´ì— ë°˜ì˜
                    p_i = torch.zeros_like(deltaL)
                    p_i[mask] = p_candidates

                    # 7. ê¸°ëŒ€ ì†ì‹¤ ê°ì†ŒëŸ‰ ê³„ì‚° ë° ìŠ¤ì¼€ì¼ ì¡°ì •
                    expected = (p_i * (-deltaL)).sum()
                    target_loss_reduction = 0.3  # ì‹¤í—˜ê°’
                    if expected > target_loss_reduction:
                        scale = target_loss_reduction / expected
                        p_i = torch.clamp(p_i * scale, max=1.0)
                else:
                    p_i = torch.zeros_like(deltaL)

                # 8. flip ê²°ì •
                flip_mask = torch.bernoulli(p_i)

                # 9. flip ì ìš© ë° ì´ì§„í™”
                weight *= (1 - 2 * flip_mask)
                weight.copy_(weight.sign())

            # === ì´ì§„ ê·¸ë˜ë””ì–¸íŠ¸ ì—…ë°ì´íŠ¸ ë ===

            running_loss += loss.item()

            # ì •í™•ë„ ê³„ì‚°
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

        # === ê²€ì¦ ë‹¨ê³„ ===
        # print("ğŸ” error_sign unique:", torch.unique(error_sign))               
        # print("ğŸ” activation unique:", torch.unique(activation))
        # print("ğŸ” weight unique:", torch.unique(weight_layer.weight))
        model.eval()
        test_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for inputs, labels in test_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, labels)

                test_loss += loss.item()
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
        
        print(f'Epoch {epoch+1} Result - Loss: {test_loss / len(test_loader):.3f}, Accuracy: {100 * correct / total:.2f}%')
        # print(model.classifier[-1].weight)
        # print(torch.norm(binary_grad))
        # print(error_sign)        

    print('Done!')       

if __name__ == '__main__':
    main()

    
